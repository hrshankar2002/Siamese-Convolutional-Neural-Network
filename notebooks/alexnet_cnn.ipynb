{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **1: Data Loading And Processing**\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define the path to the folder containing the spectrogram images\n",
    "data_dir = r'path to dataset folder'\n",
    "\n",
    "# Define the classes (folder names)\n",
    "classes = os.listdir(data_dir)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Define the desired width and height for resizing\n",
    "desired_width = 227\n",
    "desired_height = 227\n",
    "\n",
    "# Initialize lists to store spectrogram images and their corresponding labels\n",
    "images_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate through each class folder\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    # Iterate through each spectrogram image in the class folder\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(img_path)\n",
    "        # Resize the image to the desired dimensions without preserving the aspect ratio\n",
    "        img = cv2.resize(img, (desired_width, desired_height))\n",
    "        # Convert the image to float32 and normalize pixel values to [0, 1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        # Append the preprocessed image and its label to the lists\n",
    "        images_list.append(img)\n",
    "        labels_list.append(class_idx)  # Use class index as label\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images_list)\n",
    "labels = np.array(labels_list)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Shuffle the data\n",
    "indices = np.arange(images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Split the data into training, validation, and test sets (e.g., 80-10-10 split)\n",
    "train_split = int(0.8 * len(images))\n",
    "val_split = int(0.10 * len(images))\n",
    "x_train, y_train = images[:train_split], labels[:train_split]\n",
    "x_val, y_val = images[train_split:train_split + val_split], labels[train_split:train_split + val_split]\n",
    "x_test, y_test = images[train_split + val_split:], labels[train_split + val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv2D(filters=96, kernel_size=(11, 11), \n",
    "                        strides=(4, 4), activation=\"relu\", \n",
    "                        input_shape=(227, 227, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D(pool_size=(3, 3), strides= (2, 2)))\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(5, 5), \n",
    "                        strides=(1, 1), activation=\"relu\", \n",
    "                        padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n",
    "                        strides=(1, 1), activation=\"relu\", \n",
    "                        padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n",
    "                        strides=(1, 1), activation=\"relu\", \n",
    "                        padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), \n",
    "                        strides=(1, 1), activation=\"relu\", \n",
    "                        padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(17, activation=\"softmax\"))\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.optimizers.SGD(learning_rate=0.001), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **3:Training**\"\"\"\n",
    "\n",
    "# Define number of epochs and batch size\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model\n",
    "history =model.fit(\n",
    "    x_train,  # Pass the left and right images as inputs\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val),  # Pass validation data\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class indices\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "precision = precision_score(y_val, y_pred_classes, average='macro')  # Use 'weighted' if class imbalance exists\n",
    "recall = recall_score(y_val, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_val, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **4:Evaluation**\"\"\"\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Use the trained model to predict classes for the test set\n",
    "y_pred =model.predict(x_test,)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "f2 = fbeta_score(y_test, y_pred_classes, average='weighted', beta=2)\n",
    "\n",
    "print(\"Accuracy: %.5f\", accuracy)\n",
    "print(\"Precision: %.5f\", precision)\n",
    "print(\"Recall: %.5f\", recall)\n",
    "print(\"F1-score: %.5f\", f1)\n",
    "print(\"F2-score: %.5f\", f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **5:Confusion Matrix**\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred_classes are defined\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Generate normalized confusion matrix\n",
    "conf_matrix_normalized = confusion_matrix(y_test, y_pred_classes, normalize='true')\n",
    "\n",
    "# Set up subplots to display both matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot normal confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "axes[0].set_xlabel('Predicted Labels')\n",
    "axes[0].set_ylabel('True Labels')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=axes[1])\n",
    "axes[1].set_title('Normalized Confusion Matrix (Proportions)')\n",
    "axes[1].set_xlabel('Predicted Labels')\n",
    "axes[1].set_ylabel('True Labels')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"conf.png\", dpi=650)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **8:PREDICTION**\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the external spectrogram\n",
    "external_spectrogram_path = '/content/siamese_ecg_prod/dataset/DatasetWave_2/4/577.png'\n",
    "external_spectrogram = cv2.imread(external_spectrogram_path)\n",
    "\n",
    "# Preprocess the external spectrogram\n",
    "desired_width = 224\n",
    "desired_height = 224\n",
    "external_spectrogram_resized = cv2.resize(external_spectrogram, (desired_width, desired_height))\n",
    "external_spectrogram_normalized = external_spectrogram_resized.astype(np.float32) / 255.0\n",
    "\n",
    "# Reshape the input to match model's input shape (add batch dimension)\n",
    "input_external = np.expand_dims(external_spectrogram_normalized, axis=0)\n",
    "\n",
    "# Use the trained model to predict classes for the external spectrogram\n",
    "predictions = siamese_model.predict([input_external, input_external])\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Print the predicted class label\n",
    "print(\"Predicted class:\", classes[predicted_class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Assuming y_test and y_pred_proba are defined, and num_classes is the number of classes\n",
    "# Binarize the output for multi-class ROC\n",
    "y_pred_proba = siamese_model.predict([x_test, x_test])\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(num_classes))\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "# Plot micro-average ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_micro, tpr_micro, color='blue', lw=2,\n",
    "         label='Micro-average ROC curve (AUC = %0.2f)' % roc_auc_micro)\n",
    "\n",
    "# Plot diagonal line for random guessing\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Set limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Micro-average Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc.png', dpi= 650)\n",
    "plt.show()\n",
    "\n",
    "# Print overall micro-average AUC\n",
    "# print(f'Micro-average AUC: {roc_auc_micro:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **10: Accuracy Curve**\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training and validation accuracy from history\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot accuracy curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('acc.png', dpi=650)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
