{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a3bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430755c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as scio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_path = './'\n",
    "dataset_path =  r'path to dataset folder' \n",
    "\n",
    "classes = ['NSR', 'APB', 'AFL', 'AFIB', 'SVTA', 'WPW','PVC', 'Bigeminy', 'Trigeminy', \n",
    "           'VT', 'IVR', 'VFL', 'Fusion', 'LBBBB', 'RBBBB', 'SDHB', 'PR']\n",
    "ClassesNum = len(classes)\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path, topdown=False):\n",
    "    for name in files:\n",
    "    \n",
    "        data_train = scio.loadmat(os.path.join(root, name))\n",
    "        \n",
    "        # arr -> list\n",
    "        data_arr = data_train.get('val')\n",
    "        \n",
    "        data_list = data_arr.tolist()\n",
    "       \n",
    "        X.append(data_list[0]) # [[……]] -> [ ]\n",
    "        y.append(int(os.path.basename(root)[0:2]) - 1)  # name -> num\n",
    "   \n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "        \n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "    \n",
    "X=np.array(X) # (1000, 3600)\n",
    "y=np.array(y) # (1000, )\n",
    "\n",
    "X = standardization(X)\n",
    "\n",
    "X = X.reshape((744,1,3600))\n",
    "y = y.reshape((744))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"X_train : \", len(X_train))\n",
    "print(\"X_test  : \", len(X_test))\n",
    "print(\"shape of X_train : \", np.shape(X_train[0]))\n",
    "print(\"shape of y_train : \", np.shape(y_train))\n",
    "print(\"shape of X_test : \", np.shape(X_test))\n",
    "print(\"shape of y_test : \", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dea7d6",
   "metadata": {},
   "source": [
    "***(1000, 1, 3600) --> (samples, channels, time points)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a1fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = X_train.shape[0] \n",
    "        self.x_train = torch.from_numpy(X_train).float().to(\"cuda\")\n",
    "        self.y_train = torch.from_numpy(y_train).long().to(\"cuda\")\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_train[index], self.y_train[index] \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = X_test.shape[0] \n",
    "        self.x_test = torch.from_numpy(X_test).float().to(\"cuda\")\n",
    "        self.y_test = torch.from_numpy(y_test).long().to(\"cuda\")\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_test[index], self.y_test[index] \n",
    "    def __len__(self):\n",
    "        return self.len    \n",
    "        \n",
    "train_dataset = MyDataset()\n",
    "test_dataset = TestDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4be763",
   "metadata": {},
   "source": [
    "## Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_ = 1\n",
    "num_segments_in_record = 100\n",
    "segment_len = 3600   # 3600 \n",
    "num_classes = 17\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "class arrhythmia_classifier(nn.Module):\n",
    "    def __init__(self, in_channels=in_channels_):\n",
    "        super(arrhythmia_classifier, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1,8,16,stride=2,padding=7),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(8),\n",
    "            nn.MaxPool1d(kernel_size=8,stride=4),\n",
    "   \n",
    "            nn.Conv1d(8,12,12,padding=5,stride=2),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.MaxPool1d(4,stride=2),\n",
    "            \n",
    "            nn.Conv1d(12,32,9,stride=1,padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5,stride=2),\n",
    "            \n",
    "            nn.Conv1d(32,64,7,stride=1,padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4,stride=2),\n",
    "            \n",
    "            nn.Conv1d(64,64,5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,2),\n",
    "            \n",
    "            nn.Conv1d(64,64,3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,2),\n",
    "            \n",
    "            nn.Conv1d(64,72,3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,2),\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=216, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.1),\n",
    "            nn.Linear(in_features=64, out_features=17),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, ex_features=None):\n",
    "        return self.cnn(x)\n",
    "\n",
    "\n",
    "def calc_next_len_conv1d(current_len=112500, kernel_size=16, stride=8, padding=0, dilation=1):\n",
    "    return int(np.floor((current_len + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = arrhythmia_classifier().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d12bce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd698746",
   "metadata": {},
   "source": [
    "## Construct Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b3bdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay = 0.0, amsgrad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc00ea",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f8045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_list = []\n",
    "loss_list = []\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total_train += target.size(0)\n",
    "        correct_train += (predicted == target).sum().item()\n",
    "        \n",
    "        # Print running loss every 10 batches\n",
    "        if batch_idx % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.8f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            loss_list.append(running_loss / 300)  # Append average loss\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Calculate and print training accuracy after the entire epoch\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    print('Training Accuracy for epoch %d: %.8f %%' % (epoch + 1, train_accuracy))\n",
    "    return train_accuracy\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    correct_list.append(accuracy)\n",
    "    print('Accuracy on test set: %.8f %%' % accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0\n",
    "for epoch in range(200):\n",
    "    train_acc = train(epoch)\n",
    "    test_acc = test()\n",
    "    \n",
    "    if best_acc < test_acc:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        best_acc = test_acc\n",
    "\n",
    "    # Print training accuracy for the current epoch\n",
    "    print('Training Accuracy: %.8f %%' % train_acc)\n",
    "    print('Test Accuracy: %.8f %%' % test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daabaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss vs epoch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.plot(loss_list, 'b')\n",
    "plt.title('CNN: Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('loss_vs_epoch.png', dpi=650)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_model = arrhythmia_classifier().to(device)\n",
    "test_model.load_state_dict(torch.load(r'path to saved model'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy on test set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as scio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_path = './'\n",
    "dataset_path =  'path to dataset folder' \n",
    "\n",
    "classes = ['NSR', 'APB', 'AFL', 'AFIB', 'SVTA', 'WPW','PVC', 'Bigeminy', 'Trigeminy', \n",
    "           'VT', 'IVR', 'VFL', 'Fusion', 'LBBBB', 'RBBBB', 'SDHB', 'PR']\n",
    "ClassesNum = len(classes)\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path, topdown=False):\n",
    "    for name in files:\n",
    "    \n",
    "        data_train = scio.loadmat(os.path.join(root, name))\n",
    "        \n",
    "        # arr -> list\n",
    "        data_arr = data_train.get('val')\n",
    "        \n",
    "        data_list = data_arr.tolist()\n",
    "       \n",
    "        X.append(data_list[0]) # [[……]] -> [ ]\n",
    "        y.append(int(os.path.basename(root)[0:2]) - 0)  # name -> num\n",
    "   \n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "        \n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "    \n",
    "X=np.array(X) # (1000, 3600)\n",
    "y=np.array(y) # (1000, )\n",
    "\n",
    "X = standardization(X)\n",
    "\n",
    "X_val = X.reshape((4464,1,3600))\n",
    "y_val = y.reshape((4464))\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = X_val.shape[0] \n",
    "        self.x_val = torch.from_numpy(X_val).float().to(\"cuda\")\n",
    "        self.y_val = torch.from_numpy(y_val).long().to(\"cuda\")\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_val[index], self.y_val[index] \n",
    "    def __len__(self):\n",
    "        return self.len    \n",
    "        \n",
    "val_dataset = ValDataset()\n",
    "val_loader = DataLoader(dataset=val_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_model = arrhythmia_classifier().to(device)\n",
    "test_model.load_state_dict(torch.load('path to saved model'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "print('Accuracy on val set: %.5f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4732d26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['axes.grid'] = False\n",
    "plt.plot(correct_list,'r')\n",
    "plt.title('CNN: Accuracy vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "plt.savefig('accuracy vs epoch.png',dpi=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        preds = output.argmax(dim=1, keepdim=True)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())  # Flatten predictions to 1D array\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Determine unique classes from the data\n",
    "unique_classes = np.unique(np.concatenate((all_labels, all_preds)))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting confusion matrices\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Non-normalized confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('confusion_matrix4.png', dpi=650)\n",
    "plt.show()\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.5f}')\n",
    "print(f'F1 Score: {f1:.5f}')\n",
    "print(f'Precision: {precision:.5f}')\n",
    "print(f'Recall: {recall:.5f}')\n",
    "\n",
    "# Print detailed classification report\n",
    "target_names = [f'Class {i}' for i in unique_classes]\n",
    "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "print('Classification Report:\\n', report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        preds = output.argmax(dim=1, keepdim=True)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())  # Flatten predictions to 1D array\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Determine unique classes from the data\n",
    "unique_classes = np.unique(np.concatenate((all_labels, all_preds)))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting confusion matrices\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Non-normalized confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('conf.png', dpi=650)\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "# Print overall metrics\n",
    "print(f'Overall Accuracy: {accuracy:.5f}')\n",
    "print(f'F1 Score: {f1:.5f}')\n",
    "print(f'Precision: {precision:.5f}')\n",
    "print(f'Recall: {recall:.5f}')\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Print detailed classification report with class accuracies\n",
    "print('\\nClassification Report:')\n",
    "print(f\"{'Class':<10}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}{'Accuracy':<10}\")\n",
    "print('-' * 50)\n",
    "for i, cls in enumerate(unique_classes):\n",
    "    precision = precision_score(all_labels, all_preds, labels=[cls], average='micro')\n",
    "    recall = recall_score(all_labels, all_preds, labels=[cls], average='micro')\n",
    "    f1 = f1_score(all_labels, all_preds, labels=[cls], average='micro')\n",
    "    accuracy = class_accuracies[i]\n",
    "    print(f\"{cls:<10}{precision:<10.2f}{recall:<10.2f}{f1:<10.2f}{accuracy:<10.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4a1cc",
   "metadata": {},
   "source": [
    "Saliency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d9021dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(test_loader))\n",
    "\n",
    "# Assuming your model is defined as `model`\n",
    "device = next(model.parameters()).device  # Get device from model parameters\n",
    "data = data.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c496ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store true labels and prediction scores\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        all_scores.append(output.cpu().numpy())\n",
    "        all_labels.append(target.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_scores = np.concatenate(all_scores, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Binarize the labels for multi-class\n",
    "y_test_binarized = label_binarize(all_labels, classes=unique_classes)\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], all_scores[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), all_scores.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'purple', 'brown', 'gray'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {unique_classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=4,\n",
    "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec855b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store true labels and prediction scores\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        all_scores.append(output.cpu().numpy())\n",
    "        all_labels.append(target.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_scores = np.concatenate(all_scores, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Ensure that the shapes are consistent\n",
    "print(\"Shape of all_scores:\", all_scores.shape)\n",
    "print(\"Shape of all_labels:\", all_labels.shape)\n",
    "\n",
    "# Binarize the labels for multi-class\n",
    "unique_classes = np.unique(all_labels)  # Make sure unique_classes are defined\n",
    "y_test_binarized = label_binarize(all_labels, classes=unique_classes)\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Ensure consistency\n",
    "assert y_test_binarized.shape[0] == all_scores.shape[0], \"Mismatch in number of samples between binarized labels and scores\"\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], all_scores[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), all_scores.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'purple', 'brown', 'gray'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {unique_classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=4,\n",
    "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store true labels and prediction scores\n",
    "all_labels = []\n",
    "all_scores = []\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        all_scores.append(output.cpu().numpy())\n",
    "        all_labels.append(target.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_scores = np.concatenate(all_scores, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Binarize the labels for multi-class\n",
    "y_test_binarized = label_binarize(all_labels, classes=unique_classes)\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "# Compute ROC curve and AUC for micro-average\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), all_scores.ravel())\n",
    "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "# Plotting the micro-average ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_micro, tpr_micro, color='deeppink', linestyle=':', linewidth=4,\n",
    "         label=f'Micro-average ROC curve (area = {roc_auc_micro:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Overall ROC Curve with AUC (Micro-Average)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('roc_curve.png', dpi=650)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "def test_with_scores():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)  # For multiclass classification\n",
    "            \n",
    "            # Storing true labels and prediction scores\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probs.cpu().numpy())  # Store probabilities for all classes\n",
    "\n",
    "    return np.array(y_true), np.array(y_scores)\n",
    "\n",
    "# Call test_with_scores after the training loop\n",
    "y_true, y_scores = test_with_scores()\n",
    "\n",
    "# Binarize the labels for multiclass precision-recall computation (one-vs-rest)\n",
    "num_classes = y_scores.shape[1]\n",
    "y_true_bin = label_binarize(y_true, classes=[i for i in range(num_classes)])\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    plt.plot(recall, precision, marker='.', label=f'Class {i}')\n",
    "\n",
    "plt.title('Precision-Recall Curve (One-vs-Rest)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig('precision_recall_curve_multiclass.png', dpi=650)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "def test_with_scores():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)  # Get probabilities for all classes\n",
    "            \n",
    "            # Storing true labels and prediction scores\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probs.cpu().numpy())  # Store probabilities for all classes\n",
    "\n",
    "    return np.array(y_true), np.array(y_scores)\n",
    "\n",
    "# Call test_with_scores after the training loop\n",
    "y_true, y_scores = test_with_scores()\n",
    "\n",
    "# Binarize the labels for multiclass precision-recall computation\n",
    "num_classes = y_scores.shape[1]\n",
    "y_true_bin = label_binarize(y_true, classes=[i for i in range(num_classes)])\n",
    "\n",
    "# Compute precision-recall curve and average precision score using micro averaging\n",
    "precision, recall, _ = precision_recall_curve(y_true_bin.ravel(), y_scores.ravel())\n",
    "average_precision = average_precision_score(y_true_bin, y_scores, average=\"micro\")\n",
    "\n",
    "# Plotting overall precision-recall curve\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title(f'Overall Precision-Recall Curve (micro-average), AP={average_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('overall_precision_recall_curve1.png', dpi=650)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize y_true and y_scores as lists\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "def test_with_scores():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)  # Get probabilities for all classes\n",
    "            \n",
    "            # Storing true labels and prediction scores\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probs.cpu().numpy())  # Store probabilities for all classes\n",
    "\n",
    "    return np.array(y_true), np.array(y_scores)\n",
    "\n",
    "# Call test_with_scores after the training loop\n",
    "y_true, y_scores = test_with_scores()\n",
    "\n",
    "# Binarize the labels for multiclass evaluation\n",
    "num_classes = y_scores.shape[1]\n",
    "y_true_bin = label_binarize(y_true, classes=[i for i in range(num_classes)])\n",
    "\n",
    "# Compute ROC Curve and AUC Score using micro averaging\n",
    "fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_scores.ravel())\n",
    "roc_auc = roc_auc_score(y_true_bin, y_scores, average=\"micro\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.title(f'Overall ROC Curve (micro-average), AUC={roc_auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.savefig('overall_roc_curve.png', dpi=650)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
